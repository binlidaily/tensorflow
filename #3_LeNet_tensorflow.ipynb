{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Import data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# import keras\n",
    "# mnist = keras.datasets.mnist\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# mnist = input_data.read_data_sets('data/', one_hot=True)   # 读取数据集 这个 one_hot=True 要慎重选啊\n",
    "mnist = input_data.read_data_sets('data/', reshape=False)   # 读取数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./images/18-Figure2.3-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取训练数据及测试数据\n",
    "train_data, train_label = mnist.train.images, mnist.train.labels\n",
    "train_data = np.pad(train_data, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "test_data, test_label = mnist.test.images, mnist.test.labels\n",
    "test_data = np.pad(test_data, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "\n",
    "# 打乱训练数据及测试数据\n",
    "train_image_num = len(train_data)\n",
    "train_image_index = np.arange(train_image_num)\n",
    "np.random.shuffle(train_image_index)\n",
    "train_data = train_data[train_image_index]\n",
    "train_label = train_label[train_image_index]\n",
    "\n",
    "test_image_num = len(test_data)\n",
    "test_image_index = np.arange(test_image_num)\n",
    "np.random.shuffle(test_image_index)\n",
    "test_data = test_data[test_image_index]\n",
    "test_label = test_label[test_image_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义输入输出\n",
    "w, h, c = 32, 32, 1\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, w, h, c], name='x')\n",
    "y_ = tf.placeholder(tf.int32, [None], name='y_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./images/18-Figure2.3-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_network(input_tensor, regularizer, is_train=True):\n",
    "    # 第一层：卷积层，过滤器的尺寸为 5×5，深度为 6，不使用全 0 补充，步长为 1。\n",
    "    # 尺寸变化：32×32×1->28×28×6\n",
    "    with tf.variable_scope('layer1-conv1'):\n",
    "        conv1_weights = tf.get_variable('weight', [5, 5, c, 6], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_biases = tf.get_variable('bias', [6], initializer=tf.constant_initializer(0.0))\n",
    "        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding='VALID')\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "    \n",
    "    # 第二层：池化层，过滤器的尺寸为 2×2，使用全 0 补充，步长为 2。\n",
    "    # 尺寸变化：28×28×6->14×14×6\n",
    "    with tf.variable_scope('layer2-pool1'):\n",
    "        pool1 = tf.nn.max_pool(relu1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "    # 第三层：卷积层，过滤器的尺寸为 5×5，深度为 16，不使用全 0 补充，步长为 1。\n",
    "    # 尺寸变化：14×14×6->10×10×16\n",
    "    with tf.variable_scope('layer3-conv2'):\n",
    "        conv2_weights = tf.get_variable('weight', [5, 5, 6, 16], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biases = tf.get_variable('bias', [16], initializer=tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding='VALID')\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "        \n",
    "    # 第四层：池化层，过滤器的尺寸为2×2，使用全0补充，步长为2。\n",
    "    # 尺寸变化：10×10×6->5×5×16\n",
    "    with tf.variable_scope('layer4-pool2'):\n",
    "        pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    # 将第四层池化层的输出转化为第五层全连接层的输入格式。第四层的输出为5×5×16的矩阵，然而第五层全连接层需要的输入格式\n",
    "    # 为向量，所以我们需要把代表每张图片的尺寸为5×5×16的矩阵拉直成一个长度为5×5×16的向量。\n",
    "    # 举例说，每次训练64张图片，那么第四层池化层的输出的size为(64,5,5,16),拉直为向量，nodes=5×5×16=400,尺寸size变为(64,400)\n",
    "    pool_shape = pool2.get_shape().as_list()\n",
    "    nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "    reshaped = tf.reshape(pool2, [-1, nodes])\n",
    "    \n",
    "    # 第五层：全连接层，nodes=5×5×16=400，400->120的全连接\n",
    "    # 尺寸变化：比如一组训练样本为64，那么尺寸变化为64×400->64×120\n",
    "    # 训练时，引入dropout，dropout在训练时会随机将部分节点的输出改为0，dropout可以避免过拟合问题。\n",
    "    # 这和模型越简单越不容易过拟合思想一致，和正则化限制权重的大小，使得模型不能任意拟合训练数据中的随机噪声，以此达到避免过拟合思想一致。\n",
    "    # 本文最后训练时没有采用dropout，dropout项传入参数设置成了False，因为训练和测试写在了一起没有分离，不过大家可以尝试。\n",
    "    with tf.variable_scope('layer5-fc1'):\n",
    "        fc1_weights = tf.get_variable('weight', [nodes, 120], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer is not None:\n",
    "            tf.add_to_collection('losses', regularizer(fc1_weights))\n",
    "        fc1_biases = tf.get_variable('bias', [120], initializer=tf.constant_initializer(0.1))\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n",
    "        if is_train:\n",
    "            fc = tf.nn.dropout(fc1, 0.5)\n",
    "    # 第六层：全连接层，120->84的全连接\n",
    "    # 尺寸变化：比如一组训练样本为64，那么尺寸变化为64×120->64×84\n",
    "    with tf.variable_scope('layer6-fc2'):\n",
    "        fc2_weights = tf.get_variable('weight', [120, 84], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer is not None:\n",
    "            tf.add_to_collection('losses', regularizer(fc2_weights))\n",
    "        fc2_biases = tf.get_variable('bias', [84], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1, fc2_weights) + fc2_biases)\n",
    "        if is_train:\n",
    "            fc2 = tf.nn.dropout(fc2, 0.5)\n",
    "    # 第七层：全连接层（近似表示），84->10的全连接\n",
    "    # 尺寸变化：比如一组训练样本为64，那么尺寸变化为64×84->64×10。最后，64×10的矩阵经过softmax之后就得出了64张图片分类于每种数字的概率，\n",
    "    # 即得到最后的分类结果。\n",
    "    with tf.variable_scope('layer7-fc3'):\n",
    "        fc3_weights = tf.get_variable('weight', [84, 10], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None:\n",
    "            tf.add_to_collection('losses', regularizer(fc3_weights))\n",
    "        fc3_biases = tf.get_variable('bias', [10], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        logit = tf.matmul(fc2, fc3_weights) + fc3_biases\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 正则化，交叉熵，平均交叉熵，损失函数，最小化损失函数，预测和实际equal比较，tf.equal函数会得到True或False，\n",
    "# accuracy首先将tf.equal比较得到的布尔值转为float型，即True转为1.，False转为0，最后求平均值，即一组样本的正确率。\n",
    "# 比如：一组5个样本，tf.equal比较为[True False True False False],转化为float型为[1. 0 1. 0 0],准确率为2./5=40%。\n",
    "regularizer = tf.contrib.layers.l2_regularizer(0.001)\n",
    "y = build_network(x, regularizer, False)\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=y_)\n",
    "cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.cast(tf.argmax(y, 1), tf.int32), y_)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 每次获取 batch_size 个样本进行训练或测试\n",
    "def get_batch(data, label, batch_size):\n",
    "    for start_idx in range(0, len(data) - batch_size + 1, batch_size):\n",
    "        slice_idx = slice(start_idx, start_idx + batch_size)\n",
    "        yield data[slice_idx], label[slice_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.41479869461\n",
      "train acc:  0.928114086147\n",
      "test loss:  0.236933406729\n",
      "test acc:  0.967948717949\n",
      "train loss:  0.19638391984\n",
      "train acc:  0.97440701397\n",
      "test loss:  0.165096109829\n",
      "test acc:  0.978465544872\n",
      "train loss:  0.145247616811\n",
      "train acc:  0.980409633295\n",
      "test loss:  0.125717153247\n",
      "test acc:  0.98297275641\n",
      "train loss:  0.115630554896\n",
      "train acc:  0.984229481956\n",
      "test loss:  0.106165965685\n",
      "test acc:  0.984074519231\n",
      "train loss:  0.0979186176638\n",
      "train acc:  0.986503201397\n",
      "test loss:  0.0933006422546\n",
      "test acc:  0.985376602564\n",
      "train loss:  0.086919068219\n",
      "train acc:  0.988049330617\n",
      "test loss:  0.0848905564501\n",
      "test acc:  0.986478365385\n",
      "train loss:  0.0791418511113\n",
      "train acc:  0.989104336438\n",
      "test loss:  0.0821745820964\n",
      "test acc:  0.986578525641\n",
      "train loss:  0.0741027758193\n",
      "train acc:  0.989722788126\n",
      "test loss:  0.081212790468\n",
      "test acc:  0.986077724359\n",
      "train loss:  0.068760415923\n",
      "train acc:  0.990832363213\n",
      "test loss:  0.0766292449851\n",
      "test acc:  0.98687900641\n",
      "train loss:  0.0651260902617\n",
      "train acc:  0.991432625146\n",
      "test loss:  0.0746391301688\n",
      "test acc:  0.988381410256\n"
     ]
    }
   ],
   "source": [
    "# 创建Session会话\n",
    "with tf.Session() as sess:\n",
    "    # 初始化所有变量(权值，偏置等)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 将所有样本训练10次，每次训练中以64个为一组训练完所有样本。\n",
    "    # train_num可以设置大一些。\n",
    "    train_num = 10\n",
    "    batch_size = 64\n",
    "    \n",
    "    for i in range(train_num):\n",
    "        train_loss, train_acc, batch_num = 0, 0, 0\n",
    "        for train_data_batch, train_label_batch in get_batch(train_data, train_label, batch_size):\n",
    "            _, err, acc = sess.run([train_op, loss, accuracy], feed_dict={\n",
    "                x: train_data_batch, y_: train_label_batch\n",
    "            })\n",
    "            train_loss += err\n",
    "            train_acc += acc\n",
    "            batch_num += 1\n",
    "        print('train loss: ', train_loss / batch_num)\n",
    "        print('train acc: ', train_acc / batch_num)\n",
    "\n",
    "        test_loss, test_acc, batch_num = 0, 0, 0\n",
    "        for test_data_batch, test_label_batch in get_batch(test_data, test_label, batch_size):\n",
    "            err, acc = sess.run([loss, accuracy], feed_dict={\n",
    "                x: test_data_batch, y_: test_label_batch\n",
    "            })\n",
    "            test_loss += err\n",
    "            test_acc += acc\n",
    "            batch_num += 1\n",
    "        print('test loss: ', test_loss / batch_num)\n",
    "        print('test acc: ', test_acc / batch_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
