{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/binlidaily/tensorflow/blob/master/%234_VGG_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Uri-Fmc_wt_X"
   },
   "outputs": [],
   "source": [
    "# !ls tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "j-Cab82r4_0C"
   },
   "outputs": [],
   "source": [
    "# !git clone https://binlidaily:520134lin@github.com/binlidaily/tensorflow.git\n",
    "# !ls tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "LpZlQwyk4vL9",
    "outputId": "588549b3-3729-437f-96ba-19411048a795"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Import data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# import keras\n",
    "# mnist = keras.datasets.mnist\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# mnist = input_data.read_data_sets('data/', one_hot=True)   # 读取数据集 这个 one_hot=True 要慎重选啊\n",
    "mnist = input_data.read_data_sets('./data/', reshape=False)   # 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "rWVkV3f25Ceg",
    "outputId": "9db13631-8365-4cc7-dad6-5361586db7ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28, 1) (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# 读取训练数据及测试数据\n",
    "train_data, train_label = mnist.train.images, mnist.train.labels\n",
    "test_data, test_label = mnist.test.images, mnist.test.labels\n",
    "\n",
    "# 打乱训练数据及测试数据\n",
    "n_train = len(train_data)\n",
    "i_train = range(n_train)\n",
    "np.random.shuffle(i_train)\n",
    "train_data = train_data[i_train]\n",
    "\n",
    "n_test = len(test_data)\n",
    "i_test = range(n_test)\n",
    "np.random.shuffle(i_test)\n",
    "test_data = test_data[i_test]\n",
    "\n",
    "print(np.shape(train_data), np.shape(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "g52veML1Gulp"
   },
   "outputs": [],
   "source": [
    "w, h, c = 28, 28, 1\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, w, h, c], name='x')\n",
    "y_ = tf.placeholder(tf.int32, [None], name='y_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ja4XzZTmXS-I"
   },
   "outputs": [],
   "source": [
    "def build_network(input_tensor, regularizer, is_train=True):\n",
    "    # 第一层：输入层 本来是 224x224x3, 现在是 28x28x1，卷积层的大小都是 3\n",
    "\n",
    "    with tf.variable_scope('layer1-conv12'):\n",
    "        # 28x28x1 -> 28x28x64\n",
    "        print('layer1-conv12')\n",
    "        print(input_tensor.get_shape())\n",
    "        conv1_weights = tf.get_variable('weight1', [3, 3, c, 64], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_bias = tf.get_variable('bias1', [64], initializer=tf.constant_initializer(0.0))\n",
    "        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n",
    "\n",
    "        print(relu1.get_shape())\n",
    "        # 28x28x64 -> 28x28x64\n",
    "        conv2_weights = tf.get_variable('weight2', [3, 3, 64, 64], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_bias = tf.get_variable('bias2', [64], initializer=tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.conv2d(relu1, conv2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_bias))\n",
    "\n",
    "    # 28x28x64 -> 14x14x64\n",
    "    with tf.variable_scope('layer2-pool1'):\n",
    "        print('layer2-pool1')\n",
    "        print(relu2.get_shape())\n",
    "        pool1 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    with tf.variable_scope('layer3-conv34'):\n",
    "        # 14x14x64 -> 14x14x128\n",
    "        print('layer3-conv34')\n",
    "        print(pool1.get_shape())\n",
    "        conv3_weights = tf.get_variable('weight3', [3, 3, 64, 128], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv3_bias = tf.get_variable('bias3', [128], initializer=tf.constant_initializer(0.0))\n",
    "        conv3 = tf.nn.conv2d(pool1, conv3_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu3 = tf.nn.relu(tf.nn.bias_add(conv3, conv3_bias))\n",
    "\n",
    "        # 14x14x128 -> 14x14x128\n",
    "        print(relu3.get_shape())\n",
    "        conv4_weights = tf.get_variable('weight4', [3, 3, 128, 128], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv4_bias = tf.get_variable('bias4', [128], initializer=tf.constant_initializer(0.0))\n",
    "        conv4 = tf.nn.conv2d(relu3, conv4_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu4 = tf.nn.relu(tf.nn.bias_add(conv4, conv4_bias)) \n",
    "\n",
    "    # 14x14x128 -> 7x7x128\n",
    "    with tf.variable_scope('layer4-pool2'):\n",
    "        print('layer4-pool2')\n",
    "        print(relu4.get_shape())\n",
    "        pool2 = tf.nn.max_pool(relu4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    with tf.variable_scope('layer5-conv56'):\n",
    "        # 7x7x128 -> 7x7x256\n",
    "        print('layer5-conv56')\n",
    "        print(pool2.get_shape())\n",
    "        conv5_weights = tf.get_variable('weight5', [3, 3, 128, 256], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv5_bias = tf.get_variable('bias5', [256], initializer=tf.constant_initializer(0.0))\n",
    "        conv5 = tf.nn.conv2d(pool2, conv5_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu5 = tf.nn.relu(tf.nn.bias_add(conv5, conv5_bias))\n",
    "\n",
    "        # 7x7x256 -> 7x7x256\n",
    "        print(relu5.get_shape())\n",
    "        conv6_weights = tf.get_variable('weight6', [3, 3, 256, 256], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv6_bias = tf.get_variable('bias6', [256], initializer=tf.constant_initializer(0.0))\n",
    "        conv6 = tf.nn.conv2d(relu5, conv6_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu6 = tf.nn.relu(tf.nn.bias_add(conv6, conv6_bias)) \n",
    "\n",
    "    with tf.variable_scope('layer6-pool3'):\n",
    "        # 7x7x256 -> 4x4x256\n",
    "        print('layer6-pool3')\n",
    "        print(relu6.get_shape())\n",
    "        pool3 = tf.nn.max_pool(relu6, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    with tf.variable_scope('layer7-conv78'):\n",
    "        # 4x4x256 -> 4x4x512\n",
    "        print('layer7-conv78')\n",
    "        print(pool3.get_shape())\n",
    "        conv7_weights = tf.get_variable('weight7', [3, 3, 256, 512], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv7_bias = tf.get_variable('bias7', [512], initializer=tf.constant_initializer(0.0))\n",
    "        conv7 = tf.nn.conv2d(pool3, conv7_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu7 = tf.nn.relu(tf.nn.bias_add(conv7, conv7_bias))\n",
    "\n",
    "        # 4x4x512 -> 4x4x512\n",
    "        print(relu7.get_shape())\n",
    "        conv8_weights = tf.get_variable('weight8', [3, 3, 512, 512], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv8_bias = tf.get_variable('bias8', [512], initializer=tf.constant_initializer(0.0))\n",
    "        conv8 = tf.nn.conv2d(relu7, conv8_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu8 = tf.nn.relu(tf.nn.bias_add(conv8, conv8_bias)) \n",
    "\n",
    "    # 4x4x512 -> 2x2x512\n",
    "    with tf.variable_scope('layer8-pool4'):\n",
    "        print('layer8-pool4')\n",
    "        print(relu8.get_shape())\n",
    "        pool4 = tf.nn.max_pool(relu8, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    with tf.variable_scope('layer9-conv910'):\n",
    "        # 2x2x512 -> 2x2x512\n",
    "        print('layer9-conv910')\n",
    "        print(pool4.get_shape())\n",
    "        conv9_weights = tf.get_variable('weight9', [3, 3, 512, 512], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv9_bias = tf.get_variable('bias9', [512], initializer=tf.constant_initializer(0.0))\n",
    "        conv9 = tf.nn.conv2d(pool4, conv9_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu9 = tf.nn.relu(tf.nn.bias_add(conv9, conv9_bias))\n",
    "\n",
    "        # 2x2x512 -> 2x2x512\n",
    "        print(relu9.get_shape())\n",
    "        conv10_weights = tf.get_variable('weight10', [3, 3, 512, 512], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv10_bias = tf.get_variable('bias10', [512], initializer=tf.constant_initializer(0.0))\n",
    "        conv10 = tf.nn.conv2d(relu9, conv10_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu10 = tf.nn.relu(tf.nn.bias_add(conv10, conv10_bias)) \n",
    "\n",
    "    # 2x2x512 -> 1x1x512\n",
    "    with tf.variable_scope('layer10-pool5'):\n",
    "        print('layer10-pool5')\n",
    "        print(relu10.get_shape())\n",
    "        pool5 = tf.nn.max_pool(relu10, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    print(pool5.get_shape())\n",
    "    pool_shape = pool5.get_shape().as_list()\n",
    "    print(pool_shape)\n",
    "    print(len(pool_shape))\n",
    "    n_nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "    pool5_reshaped = tf.reshape(pool5, [-1, n_nodes])\n",
    "  #   n_nodes = 1\n",
    "  #   pool5_reshaped = tf.contrib.layers.flatten(pool5)\n",
    "\n",
    "    with tf.variable_scope('layer11-fc1'):\n",
    "        print('layer11-fc1')\n",
    "        print(pool5_reshaped.get_shape())\n",
    "\n",
    "        fc1_weights = tf.get_variable('weight', [n_nodes, 4096], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer is not None:\n",
    "            tf.add_to_collection('losses', regularizer(fc1_weights))\n",
    "        fc1_biases = tf.get_variable('bias', [4096], initializer=tf.constant_initializer(0.1))\n",
    "        fc1 = tf.nn.relu(tf.matmul(pool5_reshaped, fc1_weights) + fc1_biases)\n",
    "        if is_train:\n",
    "            fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "\n",
    "    with tf.variable_scope('layer12-fc2'):\n",
    "        fc2_weights = tf.get_variable('weight', [4096, 4096], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer is not None:\n",
    "            tf.add_to_collection('losses', regularizer(fc2_weights))\n",
    "        fc2_biases = tf.get_variable('bias', [4096], initializer=tf.constant_initializer(0.1))\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1, fc2_weights) + fc2_biases)\n",
    "        if is_train:\n",
    "            fc2 = tf.nn.dropout(fc2, 0.5)\n",
    "\n",
    "    with tf.variable_scope('layer13-fc3'):\n",
    "        fc3_weights = tf.get_variable('weight', [4096, 10], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer is not None:\n",
    "            tf.add_to_collection('losses', regularizer(fc3_weights))\n",
    "        fc3_biases = tf.get_variable('bias', [10], initializer=tf.constant_initializer(0.1))\n",
    "        logit = tf.matmul(fc2, fc3_weights) + fc3_biases\n",
    "\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVDRxGHpp4e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1-conv12\n",
      "(?, 28, 28, 1)\n",
      "(?, 28, 28, 64)\n",
      "layer2-pool1\n",
      "(?, 28, 28, 64)\n",
      "layer3-conv34\n",
      "(?, 14, 14, 64)\n",
      "(?, 14, 14, 128)\n",
      "layer4-pool2\n",
      "(?, 14, 14, 128)\n",
      "layer5-conv56\n",
      "(?, 7, 7, 128)\n",
      "(?, 7, 7, 256)\n",
      "layer6-pool3\n",
      "(?, 7, 7, 256)\n",
      "layer7-conv78\n",
      "(?, 4, 4, 256)\n",
      "(?, 4, 4, 512)\n",
      "layer8-pool4\n",
      "(?, 4, 4, 512)\n",
      "layer9-conv910\n",
      "(?, 2, 2, 512)\n",
      "(?, 2, 2, 512)\n",
      "layer10-pool5\n",
      "(?, 2, 2, 512)\n",
      "(?, 1, 1, 512)\n",
      "[None, 1, 1, 512]\n",
      "4\n",
      "layer11-fc1\n",
      "(?, 512)\n"
     ]
    }
   ],
   "source": [
    "regularizer = tf.contrib.layers.l2_regularizer(0.001)\n",
    "y = build_network(x, regularizer, False)\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=y_)\n",
    "cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.cast(tf.argmax(y, 1), tf.int32), y_)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "YTDL8Ttyz5kO"
   },
   "outputs": [],
   "source": [
    "def get_batch(data, label, batch_size):\n",
    "    for start_idx in range(0, len(data) - batch_size + 1, batch_size):\n",
    "        slice_idx = slice(start_idx, start_idx + batch_size)\n",
    "        yield data[slice_idx], label[slice_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "colab_type": "code",
    "id": "EkFfrMWZ0dSa",
    "outputId": "48d4fd9a-3ddf-4627-fb8c-1576396bbc64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num 64\n",
      "batch_num 128\n",
      "batch_num 192\n",
      "batch_num 256\n",
      "batch_num 320\n",
      "batch_num 384\n",
      "batch_num 448\n",
      "batch_num 512\n",
      "batch_num 576\n",
      "batch_num 640\n",
      "batch_num 704\n",
      "batch_num 768\n",
      "batch_num 832\n",
      "training loss: 499.000016285\n",
      "train accuracy: 0.100080034924\n",
      "testing loss: 47.2670174868\n",
      "testing accuracy: 0.0995592948718\n",
      "batch_num 64\n",
      "batch_num 128\n",
      "batch_num 192\n",
      "batch_num 256\n",
      "batch_num 320\n",
      "batch_num 384\n",
      "batch_num 448\n",
      "batch_num 512\n",
      "batch_num 576\n",
      "batch_num 640\n",
      "batch_num 704\n",
      "batch_num 768\n",
      "batch_num 832\n",
      "training loss: 45.3961473361\n",
      "train accuracy: 0.098661233993\n",
      "testing loss: 43.7455349947\n",
      "testing accuracy: 0.101262019231\n",
      "batch_num 64\n",
      "batch_num 128\n",
      "batch_num 192\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    train_num = 10\n",
    "    batch_size = 64\n",
    "  \n",
    "    for i in range(train_num):\n",
    "      train_loss, train_acc, batch_num = 0, 0, 0\n",
    "      for train_data_batch, train_label_batch in get_batch(train_data, train_label, batch_size):\n",
    "          _, err, acc = sess.run([train_op, loss, accuracy], feed_dict={\n",
    "              x: train_data_batch, y_: train_label_batch\n",
    "          })\n",
    "          train_loss += err\n",
    "          train_acc += acc\n",
    "          batch_num += 1\n",
    "          if batch_num % 64 == 0:\n",
    "            print('batch_num', batch_num)\n",
    "      print('training loss:', train_loss / batch_num)\n",
    "      print('train accuracy:', train_acc / batch_num)\n",
    "\n",
    "      test_loss, test_acc, batch_num = 0, 0, 0\n",
    "      for test_data_batch, test_label_batch in get_batch(test_data, test_label, batch_size):\n",
    "          err, acc = sess.run([loss, accuracy], feed_dict={\n",
    "              x: test_data_batch, y_: test_label_batch\n",
    "          })\n",
    "          test_loss += err\n",
    "          test_acc += acc\n",
    "          batch_num += 1\n",
    "\n",
    "      print('testing loss:', test_loss / batch_num)\n",
    "      print('testing accuracy:', test_acc / batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "FA7bfxphtIM7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "#4_VGG_TF.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
